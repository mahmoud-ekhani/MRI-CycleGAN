{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional CycleGAN for MRI T1 → T2 Translation (IXI Dataset)\n",
    "\n",
    "In this notebook, we demonstrate how to:\n",
    "1. Download and prepare the **IXI MRI Dataset** (T1 & T2 images).\n",
    "2. Load and inspect slices with **Nibabel**.\n",
    "3. Build a **conditional CycleGAN** for T1→T2 image-to-image translation.\n",
    "4. Train and visualize results.\n",
    "\n",
    "> **Disclaimer**: This example is for **educational** purposes. The IXI dataset may be large, and the provided code is illustrative of the approach, not necessarily optimized for clinical use.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction & Key Concepts\n",
    "2. Environment Setup & Dependencies\n",
    "3. Download & Prepare IXI MRI Data\n",
    "4. Data Loading & Preprocessing (Slice-Based)\n",
    "5. Conditional CycleGAN Model\n",
    "6. Training Loop\n",
    "7. Visualization & Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Key Concepts\n",
    "\n",
    "### 1.1 What is CycleGAN?\n",
    "CycleGAN learns a mapping between two domains (X and Y) with **unpaired** data by enforcing a **cycle-consistency** constraint, ensuring that translating from one domain to the other and back again yields the original input.\n",
    "\n",
    "### 1.2 Conditional CycleGAN for T1→T2\n",
    "We introduce a **condition** in the generator to emphasize that T2 generation depends on T1 input. One way is to supply T1 as input channels with additional condition labels or to structure the generator to expect T1 slices as input. We'll adapt the standard CycleGAN code to highlight that T1 is the condition domain, T2 is the target domain.\n",
    "\n",
    "### 1.3 Why T1→T2?\n",
    "- Certain T2-weighted characteristics (e.g., fluid contrast) may be more diagnostically relevant.\n",
    "- Generating T2 from T1 can reduce scanning time or serve data augmentation.\n",
    "\n",
    "### 1.4 Data Note\n",
    "- The [IXI Dataset](https://brain-development.org/ixi-dataset/) contains ~600 MR volumes.\n",
    "- Each subject has T1, T2, PD, etc.\n",
    "- We’ll only focus on **T1** and **T2**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup & Dependencies\n",
    "Below we install the necessary libraries. We also need `nibabel` for reading NIfTI MRI files."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install matplotlib Pillow scikit-image\n",
    "!pip install nibabel"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download & Prepare IXI MRI Data\n",
    "Below is an example of how you might download T1 and T2 NIfTI files. The dataset can be large, so consider **downloading only a subset** for demonstration. The official source: <https://brain-development.org/ixi-dataset/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Download T1 and T2 images from IXI. (Placeholder URLs)\n",
    "# In practice, you'll likely download them manually from the official site.\n",
    "\n",
    "!mkdir -p data/IXI_T1 data/IXI_T2\n",
    "# The following lines are commented out as the dataset is large.\n",
    "# !wget -c https://brain-development.org/ixi-dataset/IxiThumbnailsT1.zip -O data/IXI_T1/IxiT1.zip\n",
    "# !unzip data/IXI_T1/IxiT1.zip -d data/IXI_T1/\n",
    "\n",
    "# !wget -c https://brain-development.org/ixi-dataset/IxiThumbnailsT2.zip -O data/IXI_T2/IxiT2.zip\n",
    "# !unzip data/IXI_T2/IxiT2.zip -d data/IXI_T2/\n",
    "\n",
    "print(\"Data directories created. Please ensure your T1 and T2 NIfTI files are placed accordingly.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories created. Please ensure your T1 and T2 NIfTI files are placed accordingly.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Preprocessing (Slice-Based)\n",
    "\n",
    "We'll load a few T1/T2 volumes, extract 2D slices, and pair them by subject if available. In a purely unpaired setting, we’d randomly select T1 slices from one subject and T2 from another. However, **since the IXI dataset typically has T1 and T2 from the same subject**, we can illustrate a simpler approach.\n",
    "\n",
    "> **Note**: Real 3D approaches exist, but for demonstration we treat each 2D slice as a separate image.\n",
    "\n",
    "### 4.1 Inspect Example Slices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Placeholder paths for demonstration\n",
    "example_t1_path = \"data/IXI_T1/subject_01_t1.nii.gz\"  # replace with actual filename\n",
    "example_t2_path = \"data/IXI_T2/subject_01_t2.nii.gz\"  # replace with actual filename\n",
    "\n",
    "def load_nifti(path):\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata()  # 3D volume\n",
    "    return data\n",
    "\n",
    "# Quick check (comment out if files not present)\n",
    "if os.path.exists(example_t1_path) and os.path.exists(example_t2_path):\n",
    "    t1_data = load_nifti(example_t1_path)\n",
    "    t2_data = load_nifti(example_t2_path)\n",
    "    slice_idx = t1_data.shape[2] // 2  # middle slice\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(t1_data[:,:,slice_idx].T, cmap=\"gray\", origin=\"lower\")\n",
    "    plt.title(\"T1 Mid Slice\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(t2_data[:,:,slice_idx].T, cmap=\"gray\", origin=\"lower\")\n",
    "    plt.title(\"T2 Mid Slice\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Example files not found. Please adjust file paths.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create PyTorch Dataset\n",
    "\n",
    "We'll create a dataset that returns **(T1_slice, T2_slice)** pairs if they belong to the same subject. If you want purely unpaired data, you can randomize the matching. We'll then transform slices into 2D tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class MRIT1T2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads T1 and T2 volumes from the same subject (or across subjects), extracts 2D slices.\n",
    "    \"\"\"\n",
    "    def __init__(self, t1_paths, t2_paths, transform=None):\n",
    "        super().__init__()\n",
    "        self.t1_paths = t1_paths\n",
    "        self.t2_paths = t2_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "        # In a real scenario, you'd match subjects by ID. For demo, assume same length.\n",
    "        self.n_subjects = len(t1_paths)\n",
    "        \n",
    "        # Pre-load volumes (optional). For large datasets, load on-the-fly.\n",
    "        self.t1_volumes = []\n",
    "        self.t2_volumes = []\n",
    "        for p1, p2 in zip(t1_paths, t2_paths):\n",
    "            vol_t1 = nib.load(p1).get_fdata()\n",
    "            vol_t2 = nib.load(p2).get_fdata()\n",
    "            self.t1_volumes.append(vol_t1)\n",
    "            self.t2_volumes.append(vol_t2)\n",
    "        \n",
    "        # For simplicity, let's assume all volumes have the same shape.\n",
    "        self.volume_shape = self.t1_volumes[0].shape\n",
    "        self.depth = self.volume_shape[2]\n",
    "        # total slices = n_subjects * depth\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_subjects * self.depth\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subj_idx = idx // self.depth\n",
    "        slice_idx = idx % self.depth\n",
    "        \n",
    "        t1_slice = self.t1_volumes[subj_idx][:,:,slice_idx]\n",
    "        t2_slice = self.t2_volumes[subj_idx][:,:,slice_idx]\n",
    "        \n",
    "        # Normalize intensities here or in transforms\n",
    "        t1_slice = (t1_slice - np.min(t1_slice)) / (np.max(t1_slice) - np.min(t1_slice) + 1e-8)\n",
    "        t2_slice = (t2_slice - np.min(t2_slice)) / (np.max(t2_slice) - np.min(t2_slice) + 1e-8)\n",
    "        \n",
    "        # Convert to PIL or remain as numpy for transforms\n",
    "        t1_slice = transforms.ToPILImage()(t1_slice[np.newaxis, ...])  # add channel dim\n",
    "        t2_slice = transforms.ToPILImage()(t2_slice[np.newaxis, ...])\n",
    "        \n",
    "        if self.transform:\n",
    "            t1_slice = self.transform(t1_slice)\n",
    "            t2_slice = self.transform(t2_slice)\n",
    "        \n",
    "        return t1_slice, t2_slice\n",
    "\n",
    "# Example usage:\n",
    "transform_2d = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Suppose we have lists of matched T1 & T2 files\n",
    "t1_list = [\"data/IXI_T1/subject_01_t1.nii.gz\"]  # etc.\n",
    "t2_list = [\"data/IXI_T2/subject_01_t2.nii.gz\"]  # etc.\n",
    "# In practice, gather them all.\n",
    "\n",
    "# dataset = MRIT1T2Dataset(t1_list, t2_list, transform=transform_2d)\n",
    "# loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conditional CycleGAN Model\n",
    "We'll adapt the previously shown CycleGAN to emphasize a condition: T1 (Domain X) → T2 (Domain Y). We'll also keep T2→T1 if we want the full cycle. If you prefer a single direction (T1→T2 only), you can simplify.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-based generator. If we wanted a fully 'conditional' approach, we might\n",
    "    concatenate an extra condition channel. For simplicity, we treat T1 as input channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1, output_channels=1, n_res_blocks=6):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = in_channels * 2\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels = in_channels * 2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_res_blocks):\n",
    "            model += [ResidualBlock(in_channels)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_channels = in_channels // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2,\n",
    "                                   padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels = in_channels // 2\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_channels, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class CycleGAN:\n",
    "    \"\"\"\n",
    "    Full CycleGAN for T1->T2 and T2->T1, including adversarial, cycle, identity losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        \n",
    "        # Generators\n",
    "        self.G_XtoY = Generator(input_channels=1, output_channels=1).to(device)\n",
    "        self.G_YtoX = Generator(input_channels=1, output_channels=1).to(device)\n",
    "\n",
    "        # Discriminators\n",
    "        self.D_X = Discriminator(input_channels=1).to(device)\n",
    "        self.D_Y = Discriminator(input_channels=1).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.opt_G = torch.optim.Adam(\n",
    "            itertools.chain(self.G_XtoY.parameters(), self.G_YtoX.parameters()),\n",
    "            lr=2e-4, betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.opt_D_X = torch.optim.Adam(self.D_X.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "        self.opt_D_Y = torch.optim.Adam(self.D_Y.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "        # Losses\n",
    "        self.criterion_GAN = nn.MSELoss()\n",
    "        self.criterion_cycle = nn.L1Loss()\n",
    "        self.criterion_identity = nn.L1Loss()\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0\n",
    "        self.lambda_id = 0.5\n",
    "\n",
    "    def set_input(self, x, y):\n",
    "        # x = T1, y = T2\n",
    "        self.real_X = x.to(self.device)\n",
    "        self.real_Y = y.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        # X->Y\n",
    "        self.fake_Y = self.G_XtoY(self.real_X)\n",
    "        self.rec_X = self.G_YtoX(self.fake_Y)\n",
    "        # Y->X\n",
    "        self.fake_X = self.G_YtoX(self.real_Y)\n",
    "        self.rec_Y = self.G_XtoY(self.fake_X)\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        # Real\n",
    "        pred_real = netD(real)\n",
    "        loss_real = self.criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "        # Fake\n",
    "        pred_fake = netD(fake.detach())\n",
    "        loss_fake = self.criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        return (loss_real + loss_fake) * 0.5\n",
    "\n",
    "    def backward_D_X(self):\n",
    "        fake_X = self.fake_X\n",
    "        self.loss_D_X = self.backward_D_basic(self.D_X, self.real_X, fake_X)\n",
    "        self.loss_D_X.backward()\n",
    "\n",
    "    def backward_D_Y(self):\n",
    "        fake_Y = self.fake_Y\n",
    "        self.loss_D_Y = self.backward_D_basic(self.D_Y, self.real_Y, fake_Y)\n",
    "        self.loss_D_Y.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        # Adversarial X->Y\n",
    "        pred_fake_Y = self.D_Y(self.fake_Y)\n",
    "        self.loss_G_XtoY = self.criterion_GAN(pred_fake_Y, torch.ones_like(pred_fake_Y))\n",
    "        # Adversarial Y->X\n",
    "        pred_fake_X = self.D_X(self.fake_X)\n",
    "        self.loss_G_YtoX = self.criterion_GAN(pred_fake_X, torch.ones_like(pred_fake_X))\n",
    "\n",
    "        # Cycle loss\n",
    "        self.loss_cycle_X = self.criterion_cycle(self.rec_X, self.real_X) * self.lambda_cycle\n",
    "        self.loss_cycle_Y = self.criterion_cycle(self.rec_Y, self.real_Y) * self.lambda_cycle\n",
    "\n",
    "        # Identity loss\n",
    "        idt_X = self.G_YtoX(self.real_X)\n",
    "        idt_Y = self.G_XtoY(self.real_Y)\n",
    "        self.loss_idt_X = self.criterion_identity(idt_X, self.real_X) * self.lambda_id\n",
    "        self.loss_idt_Y = self.criterion_identity(idt_Y, self.real_Y) * self.lambda_id\n",
    "\n",
    "        self.loss_G = (self.loss_G_XtoY + self.loss_G_YtoX +\n",
    "                       self.loss_cycle_X + self.loss_cycle_Y +\n",
    "                       self.loss_idt_X + self.loss_idt_Y)\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        # Update G\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()\n",
    "\n",
    "        # Update D_X\n",
    "        self.opt_D_X.zero_grad()\n",
    "        self.backward_D_X()\n",
    "        self.opt_D_X.step()\n",
    "\n",
    "        # Update D_Y\n",
    "        self.opt_D_Y.zero_grad()\n",
    "        self.backward_D_Y()\n",
    "        self.opt_D_Y.step()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "We'll do a simple
